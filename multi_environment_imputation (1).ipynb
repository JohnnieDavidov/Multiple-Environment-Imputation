{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4GDdxe7OHPW"
      },
      "source": [
        "# Installation requirment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJvP9Om7kSHj",
        "outputId": "6566d317-7a09-41a3-ac41-c7401109053a"
      },
      "outputs": [],
      "source": [
        "! pip install pyreadstat -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV2Jhh13OdPd"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OB-uYAaaxmPH"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import warnings\n",
        "from utils import *\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read yaml configurations for experiments\n",
        "with open('experiments_config.yaml') as f:\n",
        "    configs = yaml.load(f, Loader=yaml.FullLoader)\n",
        "\n",
        "configs = {v['id']: v for v in configs['experiments']}\n",
        "\n",
        "# default parameters for experiments\n",
        "default_config = {\n",
        "    'n_iterations': 30,\n",
        "    'test_size': 0.2,\n",
        "    'split_by_env': True,\n",
        "    'model_type': 'all',\n",
        "    'model_args': {},\n",
        "    'mu_limits': (0.1, 0.4),\n",
        "    'controlled_mus': 10\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running experiment: 0, from ./Datasets/popular2.sav\n",
            "# samples: 2000, # features: 15, # environments: 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/300 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [06:52<00:00, 13.75s/it]t]\n",
            "100%|██████████| 300/300 [06:52<00:00,  1.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running experiment: 1, from ./Datasets/dataset1reshape.csv\n",
            "# samples: 348, # features: 10, # environments: 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [01:28<00:00,  2.97s/it]s]\n",
            "100%|██████████| 300/300 [01:28<00:00,  3.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running experiment: 2, from ./Datasets/dataset2reshape.csv\n",
            "# samples: 200, # features: 33, # environments: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [02:42<00:00,  5.42s/it]s]\n",
            "100%|██████████| 300/300 [02:42<00:00,  1.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running experiment: 3, from ./Datasets/dataset3reshape.csv\n",
            "# samples: 360, # features: 12, # environments: 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [01:47<00:00,  3.58s/it]s]\n",
            "100%|██████████| 300/300 [01:47<00:00,  2.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running experiment: 4, from ./Datasets/dataset4reshape.csv\n",
            "# samples: 480, # features: 17, # environments: 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [03:48<00:00,  7.60s/it]s]\n",
            "100%|██████████| 300/300 [03:48<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running experiment: 5, from ./Datasets/dataset5reshape.csv\n",
            "# samples: 500, # features: 22, # environments: 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [02:23<00:00,  4.78s/it]s]\n",
            "100%|██████████| 300/300 [02:23<00:00,  2.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running experiment: 6, from ./Datasets/dataset6reshape.csv\n",
            "# samples: 480, # features: 62, # environments: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [12:15<00:00, 24.53s/it]t]\n",
            "100%|██████████| 300/300 [12:15<00:00,  2.45s/it]\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "for k, exp in configs.items():\n",
        "    current_config = default_config.copy()\n",
        "    current_config.update(exp)\n",
        "    \n",
        "    path_to_data = current_config.get('path')\n",
        "    data_type = current_config.get('type')\n",
        "    if data_type == 'sav':\n",
        "        df = pd.read_spss(path_to_data)\n",
        "    elif data_type == 'csv':\n",
        "        df = pd.read_csv(path_to_data)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown data type: {data_type}\")\n",
        "    \n",
        "    del current_config['id']\n",
        "    del current_config['path']\n",
        "    del current_config['type']\n",
        "    \n",
        "    print(f\"Running experiment: {k}, from {path_to_data}\")\n",
        "    print(f\"# samples: {df.shape[0]}, # features: {df.shape[1]}, # environments: {df[current_config.get('groups_column')].nunique()}\")\n",
        "    df[current_config.get('groups_column')] = df[current_config.get('groups_column')].astype('category')\n",
        "    metrics, train_probs, test_probs = bootstrapping(df, **current_config)\n",
        "\n",
        "    mean_metrics, std_metrics = compute_mean_std_metrics(metrics)\n",
        "    \n",
        "    fig1, ax1 = plt.subplots(1, 1, figsize=(10, 7))\n",
        "    error_plot(metrics, ax1)\n",
        "    plt.suptitle(f\"Dataset {k}\")\n",
        "    plt.close()\n",
        "    \n",
        "    fig2, ax2 = plt.subplots(1, 1, figsize=(10, 7))\n",
        "    mu_diff_vs_rmse_plot(metrics, train_probs, test_probs, ax2)\n",
        "    plt.suptitle(f\"Dataset {k}\")\n",
        "    plt.close()\n",
        "\n",
        "    results[k] = {\n",
        "        'mean_metrics': mean_metrics,\n",
        "        'std_metrics': std_metrics,\n",
        "        'figures': (fig1, fig2),\n",
        "        'axes': (ax1, ax2)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mean results for all experiments\n",
        "sum_mean_results = defaultdict(lambda: defaultdict(float))\n",
        "for k, v in results.items():\n",
        "    for m, val in v['mean_metrics'].items():\n",
        "        sum_mean_results['train_rmse'][m] += val['train_rmse']\n",
        "        sum_mean_results['test_rmse'][m] += val['test_rmse']\n",
        "    \n",
        "for m in sum_mean_results.keys():\n",
        "    for k in sum_mean_results[m].keys():\n",
        "        sum_mean_results[m][k] /= len(results)\n",
        "\n",
        "# std results for all experiments\n",
        "sum_std_results = defaultdict(lambda: defaultdict(float))\n",
        "for k, v in results.items():\n",
        "    for m, val in v['std_metrics'].items():\n",
        "        sum_std_results['train_rmse'][m] += val['train_rmse']\n",
        "        sum_std_results['test_rmse'][m] += val['test_rmse']\n",
        "\n",
        "for m in sum_std_results.keys():\n",
        "    for k in sum_std_results[m].keys():\n",
        "        sum_std_results[m][k] /= len(results)\n",
        "\n",
        "\n",
        "fig3, ax3 = plt.subplots(1, 1, figsize=(10, 7))\n",
        "sum_results_df = pd.DataFrame(sum_mean_results)\n",
        "sum_results_df.plot(kind='bar', ax=ax3, yerr=pd.DataFrame(sum_std_results), capsize=5)\n",
        "plt.suptitle(\"Mean RMSE for all experiments\", fontsize=24)\n",
        "\n",
        "plt.legend(fontsize=14)\n",
        "plt.xticks(rotation=45, ticks=ax3.get_xticks(), labels=[i.replace('_', ' ').capitalize() for i in sum_results_df.index], fontsize=22)\n",
        "plt.yticks(fontsize=18)\n",
        "plt.tight_layout()\n",
        "fig3.savefig('paper/figures/mean_rmse.pdf')\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save all plots\n",
        "if not os.path.exists('paper/figures'):\n",
        "    os.makedirs('paper/figures')\n",
        "for k, v in results.items():\n",
        "    # add title\n",
        "    v['figures'][0].suptitle(f\"Dataset {k+1} - Error plot\", fontsize=24)\n",
        "    v['figures'][1].suptitle(f\"Dataset {k+1} - RMSE and Distribution Divergence\", fontsize=24)\n",
        "\n",
        "    # set x axis tick labels font size\n",
        "    for i, ax in enumerate(v['axes']):\n",
        "        ax.tick_params(axis='x', labelsize=16)\n",
        "        ax.tick_params(axis='y', labelsize=16)\n",
        "        if i == 0:\n",
        "            ax.set_xlabel(\"Data Split\", fontsize=22)\n",
        "        else:    \n",
        "            ax.set_xlabel(r\"$\\Delta(\\mu)$\", fontsize=22)\n",
        "        ax.set_ylabel(ax.get_ylabel(), fontsize=18)\n",
        "        \n",
        "    v['figures'][0].savefig(f\"paper/figures/{k+1}_error_plot.pdf\")\n",
        "    v['figures'][1].savefig(f\"paper/figures/{k+1}_mu_diff_vs_rmse_plot.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
